"""
RAG Content Ingestion Pipeline

This script extracts content from book URLs, chunks the text, generates embeddings using Cohere,
and stores the vectors with metadata in Qdrant. The pipeline follows the flow:
get_all_urls → extract_text_from_url → chunk_text → embedding → create_collection → save_chunk_to_qdrant
"""
import os
import hashlib
import logging
import time
import math
import requests
import uuid
from typing import List, Dict, Optional, Tuple
from urllib.parse import urljoin, urlparse
from dataclasses import dataclass, field
from datetime import datetime
import re
import threading
from collections import deque
import argparse

# External libraries
import cohere
from qdrant_client import QdrantClient
from qdrant_client.http import models
from bs4 import BeautifulSoup
from dotenv import load_dotenv


# Load environment variables
load_dotenv()


# Data classes for the core entities
@dataclass
class TextChunk:
    """Represents a segment of book content with associated metadata."""
    id: str  # Unique identifier generated from URL and content hash for idempotency
    content: str  # The actual text content of the chunk
    source_url: str  # The original URL from which the content was extracted
    page_number: Optional[int] = None  # Page number from the original document (if available)
    section_title: Optional[str] = None  # Title of the section/chapter (if available)
    created_at: datetime = field(default_factory=datetime.now)
    word_count: int = 0  # Number of words in the content
    state: str = "created"  # State: created, embedded, stored

    def __post_init__(self):
        """Calculate word count after initialization and validate required fields."""
        if not self.content:
            raise ValueError("Content must not be empty")
        if not self.source_url:
            raise ValueError("Source URL must be a valid URL")

        if self.word_count == 0 and self.content:
            self.word_count = len(self.content.split())

        if self.word_count <= 0:
            raise ValueError("Word count must be positive")


@dataclass
class Embedding:
    """Vector representation of a text chunk generated by the Cohere model."""
    chunk_id: str  # Reference to the associated TextChunk
    vector: List[float]  # The embedding vector (expected to be 1024-dimensional for Cohere)
    model_version: str = "embed-english-v3.0"  # Version of the embedding model used
    generated_at: datetime = field(default_factory=datetime.now)

    def __post_init__(self):
        """Validate that the vector has consistent dimensions and values."""
        if not self.chunk_id:
            raise ValueError("Chunk reference must exist")

        if not self.vector:
            raise ValueError("Vector must not be empty")

        # For Cohere embeddings, the expected dimension is 1024
        if len(self.vector) != 1024:
            raise ValueError(f"Vector must have consistent dimensions (expected 1024, got {len(self.vector)})")

        # Check that all values in the vector are finite numbers
        for val in self.vector:
            if not (isinstance(val, (int, float)) and not (math.isnan(val) or math.isinf(val))):
                raise ValueError("Vector values must be finite numbers")


@dataclass
class ProcessingResult:
    """Captures the status and metrics of a pipeline execution."""
    execution_id: str  # Unique identifier for this pipeline run
    start_time: datetime  # When the pipeline started
    end_time: Optional[datetime] = None  # When the pipeline completed
    total_urls_processed: int = 0  # Number of URLs successfully processed
    total_chunks_created: int = 0  # Number of text chunks created
    total_embeddings_generated: int = 0  # Number of embeddings generated
    errors: List[str] = field(default_factory=list)  # List of any errors encountered during processing
    status: str = "running"  # Final status (e.g., "success", "partial", "failed")

    def __post_init__(self):
        """Validate that execution ID is unique and counts are non-negative."""
        if not self.execution_id:
            raise ValueError("Execution ID must be unique")

        if self.total_urls_processed < 0:
            raise ValueError("Counts must be non-negative")

        if self.total_chunks_created < 0:
            raise ValueError("Counts must be non-negative")

        if self.total_embeddings_generated < 0:
            raise ValueError("Counts must be non-negative")

    def complete(self):
        """Mark the processing as completed."""
        self.end_time = datetime.now()
        if not self.errors:
            self.status = "success"
        elif self.total_urls_processed > 0:
            self.status = "partial"
        else:
            self.status = "failed"


# Utility functions
def calculate_content_hash(url: str, content: str) -> str:
    """
    Calculate a hash for the content combined with URL to ensure idempotency.

    Args:
        url: The source URL
        content: The text content

    Returns:
        A hash string that uniquely identifies this content from this URL
    """
    content_to_hash = f"{url}||{content}".encode('utf-8')
    return hashlib.sha256(content_to_hash).hexdigest()


def load_config():
    """
    Load configuration from environment variables.

    Returns:
        A dictionary with configuration values
    """
    return {
        "cohere_api_key": os.getenv("COHERE_API_KEY"),
        "qdrant_url": os.getenv("QDRANT_URL"),
        "qdrant_api_key": os.getenv("QDRANT_API_KEY"),
        "target_url": "https://physical-ai-book-topaz.vercel.app/"
    }


def setup_logging():
    """
    Set up logging configuration.
    """
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('rag_pipeline.log'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)


def handle_error(error_msg: str, logger: logging.Logger = None):
    """
    Handle errors by logging them and optionally adding to a results object.

    Args:
        error_msg: The error message to log
        logger: Optional logger to use for logging the error
    """
    if logger:
        logger.error(error_msg)
    else:
        logging.error(error_msg)


# Placeholder for the main pipeline functions
def get_all_urls(base_url: str) -> List[str]:
    """
    Extract all URLs from the base URL using sitemap.xml.

    Args:
        base_url: The base URL to extract links from

    Returns:
        A list of URLs found on the site
    """
    sitemap_url = base_url.rstrip('/') + '/sitemap.xml'
    urls = []

    try:
        response = requests.get(sitemap_url)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'xml')
        loc_tags = []

        if soup.find('sitemapindex'):
            soup = BeautifulSoup(requests.get(soup.find('loc').text).content, 'xml')
        loc_tags = soup.find_all('loc')

        for loc in loc_tags:
            url = loc.text.strip()
            # Filter out URLs that are not part of the main site
            urls.append(url)


    except requests.RequestException as e:
        logging.error(f"Error fetching sitemap from {sitemap_url}: {str(e)}")
        # If sitemap fails, try to extract URLs from the main page
        try:
            response = requests.get(base_url)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')
            # Find all links that are relative or absolute to the same domain
            for link in soup.find_all('a', href=True):
                href = link['href']
                full_url = urljoin(base_url, href)
                if full_url.startswith(base_url):
                    if full_url not in urls:  # Avoid duplicates
                        urls.append(full_url)
        except requests.RequestException as e:
            logging.error(f"Error fetching base URL {base_url}: {str(e)}")
            return []

    return urls


def extract_text_from_url(url: str) -> str:
    """
    Extract clean text content from a single URL.

    Args:
        url: The URL to extract content from

    Returns:
        The extracted text content
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'html.parser')

        # 1. Remove "noise" elements that confuse the AI
        # Docusaurus uses <nav>, <footer>, and scripts which we don't want
        for tag in soup(["script", "style", "nav", "footer", "aside", "header"]):
            tag.decompose()

        # 2. Target the main content specifically
        # Docusaurus usually wraps the actual page content in <article> or <main>
        content_area = soup.find("article")
        if not content_area:
            content_area = soup.find("main")
        
        # Fallback: if specific tags aren't found, use the body
        if not content_area:
            content_area = soup.body

        if not content_area:
            return ""

        # Get text content from the targeted area only
        text = content_area.get_text()

        # Clean up the text (remove extra whitespace)
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = ' '.join(chunk for chunk in chunks if chunk)

        return text

    except requests.RequestException as e:
        logging.error(f"Error extracting content from {url}: {str(e)}")
        return ""
    except Exception as e:
        logging.error(f"Unexpected error extracting content from {url}: {str(e)}")
        return ""
    
def validate_and_extract_url_metadata(url: str) -> Tuple[Optional[int], Optional[str]]:
    """
    Extract metadata (page number, section title) from URL content.

    Args:
        url: The URL to extract metadata from

    Returns:
        A tuple of (page_number, section_title) or (None, None) if not found
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'html.parser')

        # Try to extract page number - look for common patterns in the URL or content
        page_number = None
        # Look for page number in URL path
        page_match = re.search(r'/page/(\d+)|/(\d+)\.html|p=(\d+)', url)
        if page_match:
            page_number = int(next(g for g in page_match.groups() if g is not None))

        # Try to extract section title from the page
        section_title = None
        title_tag = soup.find(['title', 'h1', 'h2', 'h3'])
        if title_tag:
            section_title = title_tag.get_text().strip()

        return page_number, section_title

    except requests.RequestException as e:
        logging.error(f"Error extracting metadata from {url}: {str(e)}")
        return None, None
    except Exception as e:
        logging.error(f"Unexpected error extracting metadata from {url}: {str(e)}")
        return None, None


def chunk_text(content: str, metadata: Dict,
               min_chunk_size: int = 500,
               max_chunk_size: int = 800,
               overlap_size: int = 100) -> List[TextChunk]:
    """
    Split content into configurable word segments with overlap.

    Args:
        content: The text content to chunk
        metadata: Metadata to associate with each chunk (e.g., source URL, page number)
        min_chunk_size: Minimum words per chunk
        max_chunk_size: Maximum words per chunk
        overlap_size: Overlap in words between chunks

    Returns:
        A list of TextChunk objects
    """
    if not content:
        return []

    # Get metadata values
    source_url = metadata.get('source_url', '')
    page_number = metadata.get('page_number')
    section_title = metadata.get('section_title', '')

    # Split content into words
    words = content.split()
    chunks = []

    start_idx = 0
    while start_idx < len(words):
        # Determine the end index for this chunk
        end_idx = start_idx + max_chunk_size

        # If this is the last chunk and it's too small, include it all
        if end_idx >= len(words) and len(words) - start_idx < min_chunk_size:
            end_idx = len(words)

        # If not the last chunk, try to find a good breaking point (sentence end)
        if end_idx < len(words):
            # Look for a sentence ending near the end of the chunk
            for i in range(min(end_idx, len(words) - 1), max(start_idx, end_idx - 100), -1):
                if words[i][-1] in '.!?':
                    end_idx = i + 1
                    break

        # If end_idx is still too far, just take max_chunk_size
        if end_idx - start_idx > max_chunk_size and end_idx > start_idx + min_chunk_size:
            end_idx = start_idx + max_chunk_size

        # Extract the chunk content
        chunk_content = ' '.join(words[start_idx:end_idx])

        # Create a unique ID based on source URL and content hash
        content_hash = calculate_content_hash(source_url, chunk_content)

        # Create the TextChunk with preserved metadata
        text_chunk = TextChunk(
            id=content_hash,
            content=chunk_content,
            source_url=source_url,
            page_number=page_number,
            section_title=section_title
        )

        chunks.append(text_chunk)

        # Move to the next chunk with overlap
        if end_idx >= len(words):
            # Last chunk, no more chunks needed
            break

        # Calculate next start position with overlap
        next_start_idx = end_idx - overlap_size

        # Ensure we don't go backwards if overlap is too large
        if next_start_idx <= start_idx:
            next_start_idx = end_idx  # Move to the end of current chunk if overlap would cause issues

        start_idx = next_start_idx

    # Final check to ensure we don't have chunks that are too small
    filtered_chunks = []
    for chunk in chunks:
        if len(chunk.content.split()) >= min_chunk_size or len(chunk.content) >= 50:  # Minimum reasonable size
            filtered_chunks.append(chunk)
        else:
            # If it's a very small last chunk, consider merging with the previous one
            if filtered_chunks and len(chunk.content) < 50:
                # Merge with the last chunk if it's too small
                last_chunk = filtered_chunks[-1]
                merged_content = last_chunk.content + " " + chunk.content
                merged_chunk = TextChunk(
                    id=calculate_content_hash(source_url, merged_content),
                    content=merged_content,
                    source_url=source_url,
                    page_number=page_number,
                    section_title=section_title
                )
                filtered_chunks[-1] = merged_chunk
            else:
                filtered_chunks.append(chunk)

    return filtered_chunks


# Rate limiting and usage monitoring for Cohere API calls
class CohereRateLimiter:
    def __init__(self, max_calls_per_minute=100, max_calls_per_day=1000):  # Conservative rates for free tier
        self.max_calls_per_minute = max_calls_per_minute
        self.max_calls_per_day = max_calls_per_day
        self.calls = deque()
        self.daily_calls = deque()
        self.lock = threading.Lock()

    def wait_if_needed(self):
        with self.lock:
            now = time.time()
            now_date = datetime.now().date()

            # Remove calls older than 1 minute
            while self.calls and self.calls[0] <= now - 60:
                self.calls.popleft()

            # Remove calls from previous days for daily tracking
            while self.daily_calls and datetime.fromtimestamp(self.daily_calls[0]).date() < now_date:
                self.daily_calls.popleft()

            # If we've reached the per-minute limit, wait until we can make another call
            if len(self.calls) >= self.max_calls_per_minute:
                sleep_time = 60 - (now - self.calls[0])
                if sleep_time > 0:
                    time.sleep(sleep_time)
                    # Update now after sleeping
                    now = time.time()
                    now_date = datetime.now().date()
                    # Remove calls that expired during sleep
                    while self.calls and self.calls[0] <= now - 60:
                        self.calls.popleft()
                    # Remove calls from previous days for daily tracking
                    while self.daily_calls and datetime.fromtimestamp(self.daily_calls[0]).date() < now_date:
                        self.daily_calls.popleft()

            # Check if we're approaching the daily limit
            if len(self.daily_calls) >= self.max_calls_per_day * 0.9:  # 90% of daily limit
                logging.warning(f"Approaching daily API limit: {len(self.daily_calls)}/{self.max_calls_per_day}")

                if len(self.daily_calls) >= self.max_calls_per_day:
                    logging.error(f"Daily API limit reached: {len(self.daily_calls)}/{self.max_calls_per_day}")
                    raise Exception(f"Cohere API daily limit of {self.max_calls_per_day} calls reached. Please wait until tomorrow.")

            # Record this call
            self.calls.append(now)
            self.daily_calls.append(now)

            # Log usage statistics periodically
            if len(self.daily_calls) % 100 == 0:  # Log every 100 calls
                logging.info(f"Daily API usage: {len(self.daily_calls)}/{self.max_calls_per_day} calls")


# Global rate limiter instance
cohere_rate_limiter = CohereRateLimiter(max_calls_per_minute=100)  # Adjust based on your free tier limits


def generate_embedding(text: str) -> List[float]:
    """
    Generate embedding vector for text using Cohere API.

    Args:
        text: The text to generate embeddings for

    Returns:
        A list of floats representing the embedding vector
    """
    config = load_config()
    api_key = config["cohere_api_key"]

    if not api_key:
        raise ValueError("Cohere API key not found in environment variables")

    # Wait if needed due to rate limiting
    cohere_rate_limiter.wait_if_needed()

    try:
        co = cohere.Client(api_key)
        response = co.embed(
            texts=[text],
            model="embed-english-v3.0",
            input_type="search_document"  # Optimal for RAG applications
        )
        return response.embeddings[0]  # Return the first (and only) embedding
    except Exception as e:
        logging.error(f"Error generating embedding for text: {str(e)}")
        raise


def generate_embedding_with_retry(text: str, max_retries: int = 3) -> List[float]:
    """
    Generate embedding with retry mechanism and exponential backoff.

    Args:
        text: The text to generate embeddings for
        max_retries: Maximum number of retry attempts

    Returns:
        A list of floats representing the embedding vector
    """
    config = load_config()
    api_key = config["cohere_api_key"]

    if not api_key:
        raise ValueError("Cohere API key not found in environment variables")

    last_exception = None

    for attempt in range(max_retries):
        try:
            # Wait if needed due to rate limiting
            cohere_rate_limiter.wait_if_needed()

            co = cohere.Client(api_key)
            response = co.embed(
                texts=[text],
                model="embed-english-v3.0",
                input_type="search_document"  # Optimal for RAG applications
            )
            return response.embeddings[0]  # Return the first (and only) embedding
        except Exception as e:
            last_exception = e
            logging.warning(f"Attempt {attempt + 1} failed to generate embedding: {str(e)}")
            if attempt < max_retries - 1:  # Don't sleep on the last attempt
                # Exponential backoff: wait 1s, 2s, 4s, etc.
                wait_time = min(2 ** attempt, 60)  # Cap at 60 seconds
                time.sleep(wait_time)

    # If we've exhausted all retries, raise the last exception
    logging.error(f"Failed to generate embedding after {max_retries} attempts")
    raise last_exception


def create_collection(collection_name: str):
    """
    Create a Qdrant collection for storing embeddings.

    Args:
        collection_name: Name of the collection to create
    """
    config = load_config()
    qdrant_url = config["qdrant_url"]
    qdrant_api_key = config["qdrant_api_key"]

    if not qdrant_url:
        raise ValueError("Qdrant URL not found in environment variables")

    try:
        # Initialize Qdrant client
        if qdrant_api_key:
            client = QdrantClient(url=qdrant_url, api_key=qdrant_api_key,
                                  timeout=30.0)
        else:
            client = QdrantClient(url=qdrant_url, timeout=30.0)

        # Check if collection already exists
        try:
            client.get_collection(collection_name)
            logging.info(f"Collection '{collection_name}' already exists")
            # Even if it exists, try to create the index for chunk_id field
            try:
                client.create_payload_index(
                    collection_name=collection_name,
                    field_name="chunk_id",
                    field_schema=models.PayloadSchemaType.KEYWORD
                )
                logging.info("Payload index for 'chunk_id' created successfully")
            except Exception as index_error:
                logging.debug(f"Payload index for 'chunk_id' may already exist or failed: {str(index_error)}")
            return
        except:
            # Collection doesn't exist, proceed to create it
            pass

        # Create the collection with 1024-dimensional vectors (for Cohere embeddings) and cosine distance
        client.recreate_collection(
            collection_name=collection_name,
            vectors_config=models.VectorParams(size=1024, distance=models.Distance.COSINE),
            optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000, indexing_threshold=20000),
        )

        # Create a payload index for the chunk_id field to enable filtering
        try:
            client.create_payload_index(
                collection_name=collection_name,
                field_name="chunk_id",
                field_schema=models.PayloadSchemaType.KEYWORD
            )
            logging.info("Payload index for 'chunk_id' created successfully")
        except Exception as index_error:
            logging.warning(f"Could not create payload index: {str(index_error)}. Filtering may be affected.")

        logging.info(f"Collection '{collection_name}' created successfully")

    except Exception as e:
        logging.error(f"Error creating Qdrant collection '{collection_name}': {str(e)}")
        raise


def save_chunk_to_qdrant(chunk: TextChunk, embedding: List[float]):
    """
    Save a text chunk with its embedding to Qdrant.

    Args:
        chunk: The TextChunk to save
        embedding: The embedding vector to save with the chunk
    """
    config = load_config()
    qdrant_url = config["qdrant_url"]
    qdrant_api_key = config["qdrant_api_key"]
    collection_name = "rag_embedding"

    if not qdrant_url:
        raise ValueError("Qdrant URL not found in environment variables")

    try:
        # Initialize Qdrant client
        if qdrant_api_key:
            client = QdrantClient(url=qdrant_url, api_key=qdrant_api_key)
        else:
            client = QdrantClient(url=qdrant_url)

        # Use the hash string as a payload field, and generate a UUID for the point ID
        # Qdrant requires IDs to be either unsigned integers or UUIDs
        point_id = str(uuid.uuid4())  # Generate a UUID for the point ID

        # Prepare the point to be inserted
        point = models.PointStruct(
            id=point_id,  # Use a UUID for the point ID
            vector=embedding,
            payload={
                "source_url": chunk.source_url,
                "page_number": chunk.page_number,
                "section_title": chunk.section_title,
                "content": chunk.content,
                "chunk_id": chunk.id,  # Store the original chunk ID in the payload
                "created_at": chunk.created_at.isoformat(),
                "word_count": chunk.word_count
            }
        )

        # Upsert the point (insert or update if exists)
        client.upsert(
            collection_name=collection_name,
            points=[point]
        )

        # Update the chunk's state
        chunk.state = "stored"

        logging.debug(f"Chunk {chunk.id} saved to Qdrant collection '{collection_name}' with point ID {point_id}")

    except Exception as e:
        logging.error(f"Error saving chunk {chunk.id} to Qdrant: {str(e)}")
        raise


def check_chunk_exists_in_qdrant(chunk_id: str) -> bool:
    """
    Check if a chunk with the given ID already exists in Qdrant.

    Args:
        chunk_id: The ID of the chunk to check

    Returns:
        True if the chunk exists, False otherwise
    """
    config = load_config()
    qdrant_url = config["qdrant_url"]
    qdrant_api_key = config["qdrant_api_key"]
    collection_name = "rag_embedding"

    if not qdrant_url:
        raise ValueError("Qdrant URL not found in environment variables")

    try:
        # Initialize Qdrant client
        if qdrant_api_key:
            client = QdrantClient(url=qdrant_url, api_key=qdrant_api_key,
            timeout=30.0)
        else:
            client = QdrantClient(url=qdrant_url, timeout=30.0)

        # Search for records with the specific chunk_id in the payload
        # Using scroll with a filter to find records where chunk_id matches
        records, _ = client.scroll(
            collection_name=collection_name,
            scroll_filter=models.Filter(
                must=[
                    models.FieldCondition(
                        key="chunk_id",
                        match=models.MatchValue(value=chunk_id)
                    )
                ]
            ),
            limit=1
        )

        return len(records) > 0

    except Exception as e:
        logging.error(f"Error checking if chunk {chunk_id} exists in Qdrant: {str(e)}")
        # If there's an error checking, assume it doesn't exist to avoid blocking the pipeline
        return False


def process_url_content(url: str, logger: logging.Logger,
                       min_chunk_size: int = 500, max_chunk_size: int = 800, overlap_size: int = 100) -> List[TextChunk]:
    """
    Process a single URL: extract content, chunk it, and return TextChunks.

    Args:
        url: The URL to process
        logger: Logger instance for logging
        min_chunk_size: Minimum words per chunk
        max_chunk_size: Maximum words per chunk
        overlap_size: Overlap in words between chunks

    Returns:
        A list of TextChunk objects
    """
    try:
        # Extract text content from URL
        content = extract_text_from_url(url)
        if not content:
            logger.warning(f"No content extracted from {url}")
            return []

        # Extract metadata from the URL
        page_number, section_title = validate_and_extract_url_metadata(url)

        # Prepare metadata for chunking
        metadata = {
            'source_url': url,
            'page_number': page_number,
            'section_title': section_title
        }

        # Chunk the content
        chunks = chunk_text(content, metadata, min_chunk_size, max_chunk_size, overlap_size)
        logger.info(f"Successfully processed {url}, created {len(chunks)} chunks")

        return chunks

    except Exception as e:
        logger.error(f"Error processing URL {url}: {str(e)}")
        return []


def main():
    """
    Main function to execute the complete pipeline.
    """
    # Set up argument parser
    parser = argparse.ArgumentParser(description='RAG Content Ingestion Pipeline')
    parser.add_argument('--target-url', type=str, default='https://physical-ai-book-topaz.vercel.app/',
                        help='Target URL to extract content from (default: https://physical-ai-book-topaz.vercel.app/)')
    parser.add_argument('--collection-name', type=str, default='rag_embedding',
                        help='Qdrant collection name (default: rag_embedding)')
    parser.add_argument('--log-level', type=str, default='INFO',
                        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
                        help='Logging level (default: INFO)')
    parser.add_argument('--dry-run', action='store_true',
                        help='Run the pipeline without saving to Qdrant')
    parser.add_argument('--min-chunk-size', type=int, default=500,
                        help='Minimum words per chunk (default: 500)')
    parser.add_argument('--max-chunk-size', type=int, default=800,
                        help='Maximum words per chunk (default: 800)')
    parser.add_argument('--chunk-overlap', type=int, default=100,
                        help='Overlap in words between chunks (default: 100)')
    parser.add_argument('--max-api-calls-per-minute', type=int, default=100,
                        help='Maximum Cohere API calls per minute (default: 100)')
    parser.add_argument('--max-api-calls-per-day', type=int, default=1000,
                        help='Maximum Cohere API calls per day (default: 1000 for free tier)')

    args = parser.parse_args()

    # Update the global rate limiter with the configured value
    global cohere_rate_limiter
    cohere_rate_limiter = CohereRateLimiter(
        max_calls_per_minute=args.max_api_calls_per_minute,
        max_calls_per_day=args.max_api_calls_per_day
    )

    # Set up logging with specified level
    logging.basicConfig(
        level=getattr(logging, args.log_level),
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('rag_pipeline.log'),
            logging.StreamHandler()
        ]
    )
    logger = logging.getLogger(__name__)
    logger.info("RAG Content Ingestion Pipeline - Starting execution")

    print("RAG Content Ingestion Pipeline")
    print(f"Target URL: {args.target_url}")
    print(f"Collection name: {args.collection_name}")
    print(f"Dry run mode: {args.dry_run}")
    print("Starting pipeline execution...")

    # Initialize processing result
    execution_id = f"pipeline_{int(time.time())}"
    result = ProcessingResult(
        execution_id=execution_id,
        start_time=datetime.now()
    )

    try:
        # Get configuration
        config = load_config()
        target_url = args.target_url

        # Step 1: Create the Qdrant collection (unless in dry run mode)
        if not args.dry_run:
            logger.info("Creating Qdrant collection...")
            create_collection(args.collection_name)
            logger.info(f"Qdrant collection '{args.collection_name}' created successfully")
        else:
            logger.info("Dry run mode: Skipping Qdrant collection creation")

        # Step 2: Get all URLs from the target site
        logger.info(f"Extracting all URLs from {target_url}...")
        urls = get_all_urls(target_url)
        logger.info(f"Found {len(urls)} URLs to process")

        # Step 3: Process each URL and create chunks
        all_chunks = []
        for i, url in enumerate(urls):
            logger.info(f"Processing URL {i+1}/{len(urls)}: {url}")

            chunks = process_url_content(url, logger,
                                         min_chunk_size=args.min_chunk_size,
                                         max_chunk_size=args.max_chunk_size,
                                         overlap_size=args.chunk_overlap)
            all_chunks.extend(chunks)

            result.total_urls_processed += 1
            result.total_chunks_created += len(chunks)

        logger.info(f"Created a total of {len(all_chunks)} text chunks from {len(urls)} URLs")

        # Step 4: Generate embeddings and save to Qdrant
        processed_count = 0
        for chunk in all_chunks:
            try:
                # Check if chunk already exists in Qdrant (idempotency)
                if not args.dry_run and check_chunk_exists_in_qdrant(chunk.id):
                    logger.debug(f"Chunk {chunk.id} already exists in Qdrant, skipping...")
                    continue

                # Generate embedding for the chunk
                logger.debug(f"Generating embedding for chunk {chunk.id}...")
                embedding_vector = generate_embedding_with_retry(chunk.content)

                # Validate embedding dimensions
                if len(embedding_vector) != 1024:
                    logger.error(f"Invalid embedding dimensions for chunk {chunk.id}: {len(embedding_vector)}")
                    continue

                # Save chunk and embedding to Qdrant (unless in dry run mode)
                if not args.dry_run:
                    logger.debug(f"Saving chunk {chunk.id} to Qdrant...")
                    save_chunk_to_qdrant(chunk, embedding_vector)
                else:
                    logger.debug(f"Dry run mode: Would save chunk {chunk.id} to Qdrant...")

                result.total_embeddings_generated += 1
                processed_count += 1

                if processed_count % 10 == 0:  # Log progress every 10 chunks
                    logger.info(f"Processed {processed_count}/{len(all_chunks)} chunks...")

            except Exception as e:
                error_msg = f"Error processing chunk {chunk.id}: {str(e)}"
                logger.error(error_msg)
                result.errors.append(error_msg)

        logger.info(f"Successfully processed {processed_count} chunks, {result.total_embeddings_generated} embeddings saved")

    except Exception as e:
        error_msg = f"Pipeline execution failed: {str(e)}"
        logger.error(error_msg)
        result.errors.append(error_msg)

    finally:
        # Complete the processing result
        result.complete()
        logger.info(f"Pipeline execution completed with status: {result.status}")
        logger.info(f"Processed {result.total_urls_processed} URLs, "
                   f"created {result.total_chunks_created} chunks, "
                   f"generated {result.total_embeddings_generated} embeddings")

        print(f"Pipeline completed with status: {result.status}")
        print(f"Processed {result.total_urls_processed} URLs, "
              f"created {result.total_chunks_created} chunks, "
              f"generated {result.total_embeddings_generated} embeddings")

        if result.errors:
            print(f"Encountered {len(result.errors)} errors during processing")
            logger.info(f"Pipeline completed with {len(result.errors)} errors")

    return result


if __name__ == "__main__":
    main()